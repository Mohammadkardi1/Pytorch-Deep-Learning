{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "mnist_mlp_solution_with_validation.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ipuHorZKc6K",
        "colab_type": "text"
      },
      "source": [
        "# Multi-Layer Perceptron, MNIST\n",
        "---\n",
        "In this notebook, we will train an MLP to classify images from the [MNIST database](http://yann.lecun.com/exdb/mnist/) hand-written digit database.\n",
        "\n",
        "The process will be broken down into the following steps:\n",
        ">1. Load and visualize the data\n",
        "2. Define a neural network\n",
        "3. Train the model\n",
        "4. Evaluate the performance of our trained model on a test dataset!\n",
        "\n",
        "Before we begin, we have to import the necessary libraries for working with data and PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXPKh8X-Kc6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrBJoR9wKc6S",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Load Data\n",
        "You may  choose to change the `batch_size` if you want to load more data at a time.\n",
        "\n",
        "This cell will create DataLoaders for each of our datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9gL4tOPKc6U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "7f1eb61e-f088-4eb1-f9c7-7897154d3dca"
      },
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 20\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2\n",
        "\n",
        "# convert data to torch.FloatTensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# choose the training and test datasets\n",
        "train_data = datasets.MNIST(root='data', train=True,\n",
        "                                   download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='data', train=False,\n",
        "                                  download=True, transform=transform)\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "    sampler=train_sampler, num_workers=num_workers)\n",
        "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
        "    sampler=valid_sampler, num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
        "    num_workers=num_workers)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:05, 1825401.87it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 303095.49it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 6328602.67it/s]                           \n",
            "8192it [00:00, 130457.40it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nDQl-HeKc6i",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Define the Network [Architecture](http://pytorch.org/docs/stable/nn.html)\n",
        "\n",
        "The architecture will be responsible for seeing as input a 784-dim Tensor of pixel values for each image, and producing a Tensor of length 10 (our number of classes) that indicates the class scores for an input image. This particular example uses two hidden layers and dropout to avoid overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBhAd8dnKc6j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "06a014a6-1afc-4196-9a1c-18778c89ffd0"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# define the NN architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # number of hidden nodes in each layer (512)\n",
        "        hidden_1 = 512\n",
        "        hidden_2 = 256\n",
        "        # linear layer (784 -> hidden_1)\n",
        "        self.fc1 = nn.Linear(28 * 28, hidden_1)\n",
        "        # linear layer (n_hidden -> hidden_2)\n",
        "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
        "        # linear layer (n_hidden -> 10)\n",
        "        self.fc3 = nn.Linear(hidden_2, 10)\n",
        "        # dropout layer (p=0.2)\n",
        "        # dropout prevents overfitting of data\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # flatten image input\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        # add hidden layer, with relu activation function\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # add hidden layer, with relu activation function\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # add output layer\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# initialize the NN\n",
        "model = Net()\n",
        "print(model)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.2)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFaWtGP1Kc6m",
        "colab_type": "text"
      },
      "source": [
        "###  Specify [Loss Function](http://pytorch.org/docs/stable/nn.html#loss-functions) and [Optimizer](http://pytorch.org/docs/stable/optim.html)\n",
        "\n",
        "It's recommended that you use cross-entropy loss for classification. If you look at the documentation (linked above), you can see that PyTorch's cross entropy function applies a softmax funtion to the output layer *and* then calculates the log loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpBuezyOKc6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# specify loss function (categorical cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufKPITV5Kc6r",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Train the Network\n",
        "\n",
        "The steps for training/learning from a batch of data are described in the comments below:\n",
        "1. Clear the gradients of all optimized variables\n",
        "2. Forward pass: compute predicted outputs by passing inputs to the model\n",
        "3. Calculate the loss\n",
        "4. Backward pass: compute gradient of the loss with respect to model parameters\n",
        "5. Perform a single optimization step (parameter update)\n",
        "6. Update average training loss\n",
        "\n",
        "The following loop trains for 50 epochs; take a look at how the values for the training loss decrease over time. We want it to decrease while also avoiding overfitting the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMaQ0eTjKc6s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c6569f66-e2f7-40bc-fcad-15f79e156d86"
      },
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 30\n",
        "\n",
        "# initialize tracker for minimum validation loss\n",
        "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    \n",
        "\n",
        "    # train the model #\n",
        "\n",
        "    model.train() # prep model for training\n",
        "    for data, target in train_loader:\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "        \n",
        "   \n",
        "    # validate the model #\n",
        "\n",
        "    model.eval() # prep model for evaluation\n",
        "    for data, target in valid_loader:\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "        # update running validation loss \n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "        \n",
        "    # print training/validation statistics \n",
        "    # calculate average loss over an epoch\n",
        "    train_loss = train_loss/len(train_loader.sampler)\n",
        "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "    \n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch+1, \n",
        "        train_loss,\n",
        "        valid_loss\n",
        "        ))\n",
        "    \n",
        "    # save model if validation loss has decreased\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "        valid_loss_min = valid_loss"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.979948 \tValidation Loss: 0.385141\n",
            "Validation loss decreased (inf --> 0.385141).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 0.365173 \tValidation Loss: 0.297352\n",
            "Validation loss decreased (0.385141 --> 0.297352).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 0.286239 \tValidation Loss: 0.239553\n",
            "Validation loss decreased (0.297352 --> 0.239553).  Saving model ...\n",
            "Epoch: 4 \tTraining Loss: 0.238831 \tValidation Loss: 0.203710\n",
            "Validation loss decreased (0.239553 --> 0.203710).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 0.203068 \tValidation Loss: 0.180631\n",
            "Validation loss decreased (0.203710 --> 0.180631).  Saving model ...\n",
            "Epoch: 6 \tTraining Loss: 0.176846 \tValidation Loss: 0.156175\n",
            "Validation loss decreased (0.180631 --> 0.156175).  Saving model ...\n",
            "Epoch: 7 \tTraining Loss: 0.156695 \tValidation Loss: 0.143744\n",
            "Validation loss decreased (0.156175 --> 0.143744).  Saving model ...\n",
            "Epoch: 8 \tTraining Loss: 0.140702 \tValidation Loss: 0.132518\n",
            "Validation loss decreased (0.143744 --> 0.132518).  Saving model ...\n",
            "Epoch: 9 \tTraining Loss: 0.128544 \tValidation Loss: 0.120504\n",
            "Validation loss decreased (0.132518 --> 0.120504).  Saving model ...\n",
            "Epoch: 10 \tTraining Loss: 0.116056 \tValidation Loss: 0.114320\n",
            "Validation loss decreased (0.120504 --> 0.114320).  Saving model ...\n",
            "Epoch: 11 \tTraining Loss: 0.104595 \tValidation Loss: 0.106857\n",
            "Validation loss decreased (0.114320 --> 0.106857).  Saving model ...\n",
            "Epoch: 12 \tTraining Loss: 0.097673 \tValidation Loss: 0.100651\n",
            "Validation loss decreased (0.106857 --> 0.100651).  Saving model ...\n",
            "Epoch: 13 \tTraining Loss: 0.089041 \tValidation Loss: 0.097056\n",
            "Validation loss decreased (0.100651 --> 0.097056).  Saving model ...\n",
            "Epoch: 14 \tTraining Loss: 0.082633 \tValidation Loss: 0.094668\n",
            "Validation loss decreased (0.097056 --> 0.094668).  Saving model ...\n",
            "Epoch: 15 \tTraining Loss: 0.078274 \tValidation Loss: 0.091132\n",
            "Validation loss decreased (0.094668 --> 0.091132).  Saving model ...\n",
            "Epoch: 16 \tTraining Loss: 0.071594 \tValidation Loss: 0.086609\n",
            "Validation loss decreased (0.091132 --> 0.086609).  Saving model ...\n",
            "Epoch: 17 \tTraining Loss: 0.067206 \tValidation Loss: 0.085935\n",
            "Validation loss decreased (0.086609 --> 0.085935).  Saving model ...\n",
            "Epoch: 18 \tTraining Loss: 0.064059 \tValidation Loss: 0.082316\n",
            "Validation loss decreased (0.085935 --> 0.082316).  Saving model ...\n",
            "Epoch: 19 \tTraining Loss: 0.060240 \tValidation Loss: 0.079194\n",
            "Validation loss decreased (0.082316 --> 0.079194).  Saving model ...\n",
            "Epoch: 20 \tTraining Loss: 0.054727 \tValidation Loss: 0.078367\n",
            "Validation loss decreased (0.079194 --> 0.078367).  Saving model ...\n",
            "Epoch: 21 \tTraining Loss: 0.052757 \tValidation Loss: 0.076884\n",
            "Validation loss decreased (0.078367 --> 0.076884).  Saving model ...\n",
            "Epoch: 22 \tTraining Loss: 0.050056 \tValidation Loss: 0.075151\n",
            "Validation loss decreased (0.076884 --> 0.075151).  Saving model ...\n",
            "Epoch: 23 \tTraining Loss: 0.047849 \tValidation Loss: 0.076214\n",
            "Epoch: 24 \tTraining Loss: 0.044321 \tValidation Loss: 0.074285\n",
            "Validation loss decreased (0.075151 --> 0.074285).  Saving model ...\n",
            "Epoch: 25 \tTraining Loss: 0.042128 \tValidation Loss: 0.072929\n",
            "Validation loss decreased (0.074285 --> 0.072929).  Saving model ...\n",
            "Epoch: 26 \tTraining Loss: 0.039392 \tValidation Loss: 0.071172\n",
            "Validation loss decreased (0.072929 --> 0.071172).  Saving model ...\n",
            "Epoch: 27 \tTraining Loss: 0.038067 \tValidation Loss: 0.071076\n",
            "Validation loss decreased (0.071172 --> 0.071076).  Saving model ...\n",
            "Epoch: 28 \tTraining Loss: 0.036418 \tValidation Loss: 0.070818\n",
            "Validation loss decreased (0.071076 --> 0.070818).  Saving model ...\n",
            "Epoch: 29 \tTraining Loss: 0.034337 \tValidation Loss: 0.071587\n",
            "Epoch: 30 \tTraining Loss: 0.032297 \tValidation Loss: 0.070777\n",
            "Validation loss decreased (0.070818 --> 0.070777).  Saving model ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2IsM_TDKc6v",
        "colab_type": "text"
      },
      "source": [
        "###  Load the Model with the Lowest Validation Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ3W0xcCKc6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_state_dict(torch.load('model.pt'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNpj9ll9Kc6z",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Test the Trained Network\n",
        "\n",
        "Finally, we test our best model on previously unseen **test data** and evaluate it's performance. Testing on unseen data is a good way to check that our model generalizes well. It may also be useful to be granular in this analysis and take a look at how this model performs on each class as well as looking at its overall loss and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7L24V8wpKc6z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "d54a78e2-861e-43a1-b6a7-8e2a4169c053"
      },
      "source": [
        "# initialize lists to monitor test loss and accuracy\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "model.eval() # prep model for evaluation\n",
        "\n",
        "for data, target in test_loader:\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)\n",
        "    # compare predictions to true label\n",
        "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(len(target)):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# calculate and print avg test loss\n",
        "test_loss = test_loss/len(test_loader.sampler)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            str(i), 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.063014\n",
            "\n",
            "Test Accuracy of     0: 99% (971/980)\n",
            "Test Accuracy of     1: 98% (1123/1135)\n",
            "Test Accuracy of     2: 98% (1013/1032)\n",
            "Test Accuracy of     3: 98% (995/1010)\n",
            "Test Accuracy of     4: 96% (949/982)\n",
            "Test Accuracy of     5: 97% (874/892)\n",
            "Test Accuracy of     6: 97% (938/958)\n",
            "Test Accuracy of     7: 96% (996/1028)\n",
            "Test Accuracy of     8: 97% (954/974)\n",
            "Test Accuracy of     9: 98% (991/1009)\n",
            "\n",
            "Test Accuracy (Overall): 98% (9804/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}